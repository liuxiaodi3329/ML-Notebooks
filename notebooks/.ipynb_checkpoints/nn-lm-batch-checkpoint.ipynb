{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Models\n",
    "Status of Notebook: Work in Progress\n",
    "\n",
    "Reference: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dynet Version: https://github.com/neubig/nn4nlp-code/blob/master/02-lm/nn-lm.py\n",
    "\n",
    "Old PyTorch version: https://github.com/neubig/nn4nlp-code/blob/master/02-lm-pytorch/nn-lm-batch.py\n",
    "\n",
    "Additions compared to `nn.lm.ipnyb`:\n",
    "- Cleaned up model architecture code\n",
    "- Added Dropout\n",
    "- Using different initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # 导入torch模块，用于进行深度学习相关的操作\n",
    "import random # 导入random模块，用于生成随机数\n",
    "import torch # 再次导入torch模块，可能是因为后续的代码需要使用torch的功能\n",
    "import torch.nn as nn # 导入torch.nn模块，用于定义神经网络模型\n",
    "import math # 导入math模块，用于数学计算中的一些函数\n",
    "import time # 导入time模块，用于进行时间相关的操作\n",
    "import numpy as np # 导入numpy模块，用于进行科学计算中的数组操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download the datasets\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/test.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/train.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in data, pro=ess each line and split columns by \" ||| \"\n",
    "def read_data(filename):\n",
    "    data = []  # 创建一个空列表，用于存储读取的数据\n",
    "    with open(filename, \"r\") as f:  # 打开文件，以只读模式打开\n",
    "        for line in f:  # 遍历文件的每一行\n",
    "            line = line.strip().split(\" \")  # 去除行首行尾的空白字符并按空格分割得到一个列表\n",
    "            data.append(line)  # 将分割后的列表添加到data列表中\n",
    "    return data  # 返回读取的数据\n",
    "\n",
    "# 读取数据\n",
    "train_data = read_data('data/ptb/train.txt')  # 读取'train.txt'文件中的数据并赋值给train_data变量\n",
    "val_data = read_data('data/ptb/valid.txt')  # 读取'valid.txt'文件中的数据并赋值给val_data变量\n",
    "\n",
    "# 创建单词和标签索引及特殊符号\n",
    "word_to_index = {}  # 创建一个空字典，用于存储单词到索引的映射关系\n",
    "index_to_word = {}  # 创建一个空字典，用于存储索引到单词的映射关系\n",
    "word_to_index[\"<s>\"] = len(word_to_index)  # 将\"<s>\"添加到字典中，并将其索引赋值为当前字典长度\n",
    "index_to_word[len(word_to_index)-1] = \"<s>\"  # 将索引到单词的映射关系添加到字典中\n",
    "\n",
    "word_to_index[\"<unk>\"] = len(word_to_index)  # 将\"<unk>\"添加到字典中，并将其索引赋值为当前字典长度\n",
    "index_to_word[len(word_to_index)-1] = \"<unk>\"  # 将索引到单词的映射关系添加到字典中\n",
    "\n",
    "# create word to index dictionary and tag to index dictionary from data\n",
    "def create_dict(data, check_unk=False):\n",
    "    # 遍历每一行数据\n",
    "    for line in data:\n",
    "        # 遍历每个单词\n",
    "        for word in line:\n",
    "            # 检查是否需要检查未知单词\n",
    "            if check_unk == False:\n",
    "                # 如果单词不在word_to_index字典中\n",
    "                if word not in word_to_index:\n",
    "                    # 将单词添加到word_to_index字典，并赋值为当前字典的长度\n",
    "                    word_to_index[word] = len(word_to_index)\n",
    "                    # 将索引到单词的映射关系添加到index_to_word字典中\n",
    "                    index_to_word[len(word_to_index)-1] = word\n",
    "\n",
    "            # 由于数据中已经包含了<unk>，所以该分支的代码没有影响\n",
    "            # 应该只用于处理不包含<unk>的数据\n",
    "            else: \n",
    "                # 如果单词不在word_to_index字典中\n",
    "                if word not in word_to_index:\n",
    "                    # 将单词的索引赋值为\"<unk>\"的索引\n",
    "                    word_to_index[word] = word_to_index[\"<unk>\"]\n",
    "                    # 将索引到单词的映射关系添加到index_to_word字典中\n",
    "                    index_to_word[len(word_to_index)-1] = word\n",
    "\n",
    "# 调用create_dict函数，处理train_data数据\n",
    "create_dict(train_data)\n",
    "# 调用create_dict函数，处理val_data数据，同时设置check_unk为True\n",
    "create_dict(val_data, check_unk=True)\n",
    "\n",
    "# create word and tag tensors from data\n",
    "def create_tensor(data):\n",
    "    # 遍历数据中的每一行\n",
    "    for line in data:\n",
    "        # 使用列表生成式将单词转换为对应的索引，并生成一个生成器对象\n",
    "        yield [word_to_index[word] for word in line]\n",
    "\n",
    "# 调用create_tensor函数处理train_data数据，并将生成器对象转换为列表\n",
    "train_data = list(create_tensor(train_data))\n",
    "# 调用create_tensor函数处理val_data数据，并将生成器对象转换为列表\n",
    "val_data = list(create_tensor(val_data))\n",
    "\n",
    "# 计算word_to_index字典中的单词数量，即词汇量\n",
    "number_of_words = len(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation we are using batched training. There are a few differences from the original implementation found [here](https://github.com/neubig/nn4nlp-code/blob/master/02-lm/loglin-lm.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU设备，如果有则使用cuda，否则使用cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 定义N，表示n-gram的长度\n",
    "N = 2\n",
    "\n",
    "# 定义EMB_SIZE，表示嵌入层的大小\n",
    "EMB_SIZE = 128\n",
    "\n",
    "# 定义HID_SIZE，表示隐藏层的大小\n",
    "HID_SIZE = 128\n",
    "\n",
    "# Neural LM\n",
    "class FNN_LM(nn.Module):\n",
    "    def __init__(self, number_of_words, ngram_length, EMB_SIZE, HID_SIZE, dropout):\n",
    "        super(FNN_LM, self).__init__()\n",
    "\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(number_of_words, EMB_SIZE)\n",
    "\n",
    "        # 全连接前馈神经网络\n",
    "        self.fnn = nn.Sequential(\n",
    "            # 隐藏层：线性变换，将输入大小转换为隐藏层大小\n",
    "            nn.Linear(EMB_SIZE * ngram_length, HID_SIZE),\n",
    "            # 激活函数：双曲正切函数（Tanh）\n",
    "            nn.Tanh(),\n",
    "            # 丢弃层：根据丢弃概率随机丢弃元素\n",
    "            nn.Dropout(dropout),\n",
    "            # 输出层：线性变换，将隐藏层大小转换为单词数量\n",
    "            nn.Linear(HID_SIZE, number_of_words)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.embedding(x)     # 嵌入层           # Size: [batch_size x num_hist x emb_size]\n",
    "        feat = embs.view(embs.size(0), -1)  # 特征提取，将嵌入层的输出展平为一维向量  # Size: [batch_size x (num_hist*emb_size)]\n",
    "        logit = self.fnn(feat)  # 使用全连接前馈神经网络进行正向传播               # Size: batch_size x num_words                    \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建FNN_LM模型实例\n",
    "model = FNN_LM(number_of_words, N, EMB_SIZE, HID_SIZE, dropout=0.2)\n",
    "\n",
    "# 创建优化器，将模型参数作为优化器的输入，设置学习率为0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 创建损失函数，交叉熵损失函数，reduction=\"sum\"表示将所有样本的损失求和\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "# 如果GPU可用，则将模型移动到GPU设备上进行计算\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "# function to calculate the sentence loss\n",
    "def calc_sent_loss(sent):\n",
    "    # 将起始符号\"<s>\"映射为对应的整数索引\n",
    "    S = word_to_index[\"<s>\"]\n",
    "    \n",
    "    # 初始化历史记录，将每个元素都设置为起始符号的索引\n",
    "    hist = [S] * N\n",
    "    \n",
    "    # 收集所有目标词和历史记录\n",
    "    all_targets = []\n",
    "    all_histories = []\n",
    "    \n",
    "    # 遍历句子中的每个单词，包括句子末尾的标记符号\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        # 更新历史记录，将历史记录中的第一个元素删除，并将下一个单词添加到末尾\n",
    "        hist = hist[1:] + [next_word]\n",
    "\n",
    "    # 将历史记录转换为LongTensor类型，并将其移动到GPU设备上\n",
    "    logits = model(torch.LongTensor(all_histories).to(device))\n",
    "    # 将目标词转换为LongTensor类型，并将其移动到GPU设备上\n",
    "    loss = criterion(logits, torch.LongTensor(all_targets).to(device))\n",
    "\n",
    "    return loss\n",
    "\n",
    "MAX_LEN = 100\n",
    "# Function to generate a sentence\n",
    "def generate_sent():\n",
    "    # 将起始符号\"<s>\"映射为对应的整数索引\n",
    "    S = word_to_index[\"<s>\"]\n",
    "    # 初始化历史记录，将每个元素都设置为起始符号的索引\n",
    "    hist = [S] * N\n",
    "    # 初始化生成的句子列表\n",
    "    sent = []\n",
    "    while True:\n",
    "        # 使用模型进行预测，将历史记录转换为LongTensor类型，并将其移动到GPU设备上\n",
    "        logits = model(torch.LongTensor([hist]).to(device))\n",
    "        # 对预测结果进行softmax归一化\n",
    "        p = torch.nn.functional.softmax(logits)  # 形状为(1, number_of_words)\n",
    "        # 从概率分布中采样得到一个单词\n",
    "        next_word = p.multinomial(num_samples=1).item()\n",
    "        # 如果采样到了结束符号\"<s>\"，或者句子长度达到了最大长度限制，则停止生成\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        # 将生成的单词添加到句子列表中\n",
    "        sent.append(next_word)\n",
    "        # 更新历史记录，将历史记录中的第一个元素删除，并将生成的单词添加到末尾\n",
    "        hist = hist[1:] + [next_word]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences (words/sec=12807.67)\n",
      "--finished 10000 sentences (words/sec=12788.71)\n",
      "--finished 15000 sentences (words/sec=12807.44)\n",
      "--finished 20000 sentences (words/sec=12801.59)\n",
      "--finished 25000 sentences (words/sec=12852.69)\n",
      "--finished 30000 sentences (words/sec=12843.39)\n",
      "--finished 35000 sentences (words/sec=12835.04)\n",
      "--finished 40000 sentences (words/sec=12816.01)\n",
      "iter 0: train loss/word=6.1274, ppl=458.2398, (words/sec=12801.17)\n",
      "iter 0: dev loss/word=5.8676, ppl=353.3835, (words/sec=1.44s)\n",
      "it will change at georgia & co. got instead of totally a appointment from the big bankers posted <unk> & co. also received that brokers\n",
      "one and claim the politicians amount for <unk> the measure of the california santa contract\n",
      "our birth capitol led the giant <unk> by an <unk> market in the central <unk> held the rise of the company 's sheet that the irs on britain dollars\n",
      "yesterday 's jail & <unk> investigations on news for buying creditors has lower market for polish statement so and now in a bill to government americans system to my march and programs of links stock-market program charlotte nasdaq lowest judge <unk> provide an final state university of foot an woman spokesman for something he was very constitution on the new post\n",
      "it N also buy-out of bank in may industry mr. phelan said <unk> in his current N N owned of closely bartlett below minister blocking which mr. repeat and unemployment to de our own news to submit resolution trust said a rivals on the reached which now end for most cautioned failed more than N remic mortgage officials and and goldman sachs & co. currently myself mrs. mandatory almost <unk> to the hoffman <unk> greater silver kidder peabody & co. does n't <unk> any grip from the buy-out change in drexel 's third-quarter profit of N N to speculation a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences (words/sec=12587.62)\n",
      "--finished 10000 sentences (words/sec=12652.41)\n",
      "--finished 15000 sentences (words/sec=12740.18)\n",
      "--finished 20000 sentences (words/sec=12763.71)\n",
      "--finished 25000 sentences (words/sec=12753.94)\n",
      "--finished 30000 sentences (words/sec=12754.24)\n",
      "--finished 35000 sentences (words/sec=12762.18)\n",
      "--finished 40000 sentences (words/sec=12740.41)\n",
      "iter 1: train loss/word=5.7389, ppl=310.7324, (words/sec=12744.21)\n",
      "iter 1: dev loss/word=5.7766, ppl=322.6629, (words/sec=1.40s)\n",
      "the advertising for champion the dollar was named whose damage was down from lawyers and the new england told them need\n",
      "rumors with cents a share\n",
      "justice general operations in chicago\n",
      "british bought what of going to pay to rates since april\n",
      "according to an <unk> family\n",
      "--finished 5000 sentences (words/sec=12702.39)\n",
      "--finished 10000 sentences (words/sec=12731.82)\n",
      "--finished 15000 sentences (words/sec=12755.89)\n",
      "--finished 20000 sentences (words/sec=12828.83)\n",
      "--finished 25000 sentences (words/sec=12836.63)\n",
      "--finished 30000 sentences (words/sec=12801.01)\n",
      "--finished 35000 sentences (words/sec=12803.18)\n",
      "--finished 40000 sentences (words/sec=12779.24)\n",
      "iter 2: train loss/word=5.5996, ppl=270.3145, (words/sec=12792.34)\n",
      "iter 2: dev loss/word=5.7464, ppl=313.0468, (words/sec=1.40s)\n",
      "french his experience within george bush expected luxury world culture by planning\n",
      "complete as example this scheduled other sellers operations\n",
      "much of the retail\n",
      "just scheduled time to foreign exchange cigarette merchandise outlets in the market 's $ N a share a year earlier branch in the turmoil before our company\n",
      "the manufacturing products construction <unk> has about $ N million or $ N million from $ N million in bridge\n",
      "--finished 5000 sentences (words/sec=12953.86)\n",
      "--finished 10000 sentences (words/sec=12970.24)\n",
      "--finished 15000 sentences (words/sec=12896.14)\n",
      "--finished 20000 sentences (words/sec=12875.42)\n",
      "--finished 25000 sentences (words/sec=12833.31)\n",
      "--finished 30000 sentences (words/sec=12839.11)\n",
      "--finished 35000 sentences (words/sec=12822.49)\n",
      "--finished 40000 sentences (words/sec=12814.87)\n",
      "iter 3: train loss/word=5.5124, ppl=247.7381, (words/sec=12819.57)\n",
      "iter 3: dev loss/word=5.7235, ppl=305.9709, (words/sec=1.39s)\n",
      "ago\n",
      "british community interest reserves in a low $ N a tax ministry in japan and wants to say it shows the consumer price network\n",
      "expects operations for this first maryland staff studies in beijing\n",
      "u.s. government bills here have been implemented\n",
      "computer software inc. and expected that were out the <unk> moscow who illegally a <unk> <unk> stage sets for over $ N million a year earlier\n",
      "--finished 5000 sentences (words/sec=12860.84)\n",
      "--finished 10000 sentences (words/sec=12756.10)\n",
      "--finished 15000 sentences (words/sec=12795.20)\n",
      "--finished 20000 sentences (words/sec=12799.80)\n",
      "--finished 25000 sentences (words/sec=12830.27)\n",
      "--finished 30000 sentences (words/sec=12820.51)\n",
      "--finished 35000 sentences (words/sec=12821.23)\n",
      "--finished 40000 sentences (words/sec=12839.05)\n",
      "iter 4: train loss/word=5.4502, ppl=232.8159, (words/sec=12841.22)\n",
      "iter 4: dev loss/word=5.7149, ppl=303.3545, (words/sec=1.40s)\n",
      "but the pilots are not profitable if mr. <unk> said\n",
      "his trading know that cathay is through the research session mr. brooks and center usually raised its quarterly dividend\n",
      "and <unk> <unk> kept a <unk> turn on the moment he said\n",
      "but a church premium still the stockholders die\n",
      "if they benefited from the san couple has a china 's strategy while government prices\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for ITER in range(5):\n",
    "    # 进行训练\n",
    "    random.shuffle(train_data)  # 对训练数据进行随机打乱\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    train_words, train_loss = 0, 0.0  # 初始化训练数据的单词数量和损失值\n",
    "    start = time.time()  # 记录训练开始的时间\n",
    "    for sent_id, sent in enumerate(train_data):  # 遍历训练数据的每个句子\n",
    "        my_loss = calc_sent_loss(sent)  # 计算当前句子的损失\n",
    "        train_loss += my_loss.item()  # 累加损失值\n",
    "        train_words += len(sent)  # 累加单词数量\n",
    "        optimizer.zero_grad()  # 清空优化器的梯度缓存\n",
    "        my_loss.backward()  # 反向传播计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        if (sent_id+1) % 5000 == 0:  # 每处理5000个句子打印一次训练进度\n",
    "            print(\"--finished %r sentences (words/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f, (words/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "\n",
    "    # 进行评估\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    dev_words, dev_loss = 0, 0.0  # 初始化验证数据的单词数量和损失值\n",
    "    start = time.time()  # 记录评估开始的时间\n",
    "    for sent_id, sent in enumerate(val_data):  # 遍历验证数据的每个句子\n",
    "        my_loss = calc_sent_loss(sent)  # 计算当前句子的损失\n",
    "        dev_loss += my_loss.item()  # 累加损失值\n",
    "        dev_words += len(sent)  # 累加单词数量\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, (words/sec=%.2fs)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
    "\n",
    "    # Generate a few sentences\n",
    "    for _ in range(5):\n",
    "    sent = generate_sent()  # 生成一个句子\n",
    "    print(\" \".join([index_to_word[x] for x in sent]))  # 将句子中的每个单词转换为对应的词表中的词，并以空格分隔打印出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
